{"cells":[{"cell_type":"markdown","metadata":{},"source":["### one time installations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T17:59:51.498874Z","iopub.status.busy":"2021-11-13T17:59:51.497796Z","iopub.status.idle":"2021-11-13T17:59:51.512977Z","shell.execute_reply":"2021-11-13T17:59:51.512057Z","shell.execute_reply.started":"2021-11-13T17:59:51.49878Z"},"trusted":true},"outputs":[],"source":["# !gdown --id 1pb7gEkctrVrJA79EAIo7H7nuzD6uV1fW\n","# !gdown --id 1oIeAE9HXXKWPcYa-AZ0ht5ef6sKe_Vh_\n","# !gdown --id 10rAuIDvsYR2yDiCqP7GmYGPc-UmtLbJb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T17:59:51.737495Z","iopub.status.busy":"2021-11-13T17:59:51.736737Z","iopub.status.idle":"2021-11-13T17:59:51.742381Z","shell.execute_reply":"2021-11-13T17:59:51.741156Z","shell.execute_reply.started":"2021-11-13T17:59:51.73746Z"},"trusted":true},"outputs":[],"source":["# !pip install --quiet transformers --target=/kaggle/working/chaii_packages\n","# !pip install --quiet datasets --target=/kaggle/working/chaii_packages\n","# !pip install --quiet SentencePiece --target=/kaggle/working/chaii_packages\n","# !pip install --quiet pytorch-lightning --target=/kaggle/working/chaii_packages \n","# !pip install ipdb --target=/kaggle/working/chaii_packages\n","# import sys\n","# sys.path.append('/kaggle/working/chaii_packages')"]},{"cell_type":"markdown","metadata":{},"source":["### libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T17:59:52.503937Z","iopub.status.busy":"2021-11-13T17:59:52.502966Z","iopub.status.idle":"2021-11-13T17:59:52.517324Z","shell.execute_reply":"2021-11-13T17:59:52.516172Z","shell.execute_reply.started":"2021-11-13T17:59:52.503855Z"},"trusted":true},"outputs":[],"source":["# %env PYTHONPATH= \n","%env WANDB_DISABLED=True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T17:59:52.699317Z","iopub.status.busy":"2021-11-13T17:59:52.699032Z","iopub.status.idle":"2021-11-13T18:00:02.258362Z","shell.execute_reply":"2021-11-13T18:00:02.257206Z","shell.execute_reply.started":"2021-11-13T17:59:52.699287Z"},"trusted":true},"outputs":[],"source":["import os\n","import ast\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","import random\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n","from transformers import Trainer, TrainingArguments\n","from transformers import default_data_collator\n","\n","import pytorch_lightning as pl"]},{"cell_type":"markdown","metadata":{},"source":["### hyperparameters "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:02.26119Z","iopub.status.busy":"2021-11-13T18:00:02.260936Z","iopub.status.idle":"2021-11-13T18:00:02.272851Z","shell.execute_reply":"2021-11-13T18:00:02.269049Z","shell.execute_reply.started":"2021-11-13T18:00:02.261156Z"},"trusted":true},"outputs":[],"source":["class hyperparameters:\n","    # seed\n","    seed = 2021\n","    \n","    # tokenizer\n","    tokenizer_name = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\"\n","    max_len = 384 # maximum length of context and question in a datapoint\n","    overlap_len = 128 # overlap between two parts of the context when it is split\n","    \n","    # model\n","    model_name = tokenizer_name\n","    batch_size = 4\n","    epochs = 1\n","    \n","    # data\n","    train_csv = \"../input/chaii-hindi-and-tamil-question-answering/train.csv\" \n","    test_csv = \"../input/chaii-hindi-and-tamil-question-answering/test.csv\"\n","    external_mlqa = \"../input/external-data-mlqa-xquad-preprocessing/mlqa_hindi.csv\"\n","    # external_csv2 = \"../input/external-data-mlqa-xquad-preprocessing/xquad.csv\"\n","    # external_csv3 = \"../input/squad-translated-to-tamil-for-chaii/squad_translated_tamil.csv\"\n","    external_google_hi = \"../input/google-translated-squad20-to-hindi-and-tamil/squad_hi.csv\"\n","    external_google_ta = \"../input/google-translated-squad20-to-hindi-and-tamil/squad_ta.csv\"\n","    \n","    # prediction\n","    top_x = 20 # top 5 answer predictions by each feature\n","    max_tok_in_ans = 30 # max 10 tokens in predicted answer text\n","    output_dir = \"model_dir\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:02.274984Z","iopub.status.busy":"2021-11-13T18:00:02.274523Z","iopub.status.idle":"2021-11-13T18:00:02.28404Z","shell.execute_reply":"2021-11-13T18:00:02.283005Z","shell.execute_reply.started":"2021-11-13T18:00:02.274938Z"},"trusted":true},"outputs":[],"source":["hyperparams = hyperparameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:02.28773Z","iopub.status.busy":"2021-11-13T18:00:02.28734Z","iopub.status.idle":"2021-11-13T18:00:02.349684Z","shell.execute_reply":"2021-11-13T18:00:02.348575Z","shell.execute_reply.started":"2021-11-13T18:00:02.287671Z"},"trusted":true},"outputs":[],"source":["# pl.seed_everything(hyperparams.seed)\n","print(\"available gpu count:\", torch.cuda.device_count())\n","for i in range(torch.cuda.device_count()):\n","    print(torch.cuda.device(i))"]},{"cell_type":"markdown","metadata":{},"source":["### tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:02.35507Z","iopub.status.busy":"2021-11-13T18:00:02.352273Z","iopub.status.idle":"2021-11-13T18:00:03.401562Z","shell.execute_reply":"2021-11-13T18:00:03.400488Z","shell.execute_reply.started":"2021-11-13T18:00:02.355019Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(hyperparams.tokenizer_name)"]},{"cell_type":"markdown","metadata":{},"source":["### Ingredients (data) for Chaii"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:03.405595Z","iopub.status.busy":"2021-11-13T18:00:03.404891Z","iopub.status.idle":"2021-11-13T18:00:03.804816Z","shell.execute_reply":"2021-11-13T18:00:03.803817Z","shell.execute_reply.started":"2021-11-13T18:00:03.405559Z"},"trusted":true},"outputs":[],"source":["extdata = pd.read_csv(hyperparams.external_mlqa,encoding = 'utf-8')\n","# extdata2 = pd.read_csv(hyperparams.external_csv2,encoding = 'utf-8')\n","# extdata3 = pd.read_csv(hyperparams.external_csv3,encoding = 'utf-8')\n","# extdata3[\"language\"] = ['tamil']*len(extdata3)\n","# extdata = pd.concat([extdata1, extdata2,extdata3])\n","extdata['id'] = list(np.arange(1, len(extdata)+1))\n","# chaii_df_1 = pd.read_csv(hyperparams.train_csv, encoding='utf-8')\n","# chaii_df = pd.concat([chaii_df_1, extdata]).reset_index(drop=True)\n","# chaii_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:03.80796Z","iopub.status.busy":"2021-11-13T18:00:03.807328Z","iopub.status.idle":"2021-11-13T18:00:03.819403Z","shell.execute_reply":"2021-11-13T18:00:03.818101Z","shell.execute_reply.started":"2021-11-13T18:00:03.807878Z"},"trusted":true},"outputs":[],"source":["def convert_answers(row):\n","    return {'answer_start': [row[0]['answer_start']], 'text': [row[0]['text']]}\n","\n","def process_google_translate(df, hindi=True):\n","    df = df.loc[df.loc[:, 'is_in'] == True].reset_index(drop=True)\n","    df['answers'] = df['answers'].apply(ast.literal_eval)\n","    df['answers'] = df['answers'].apply(convert_answers)\n","    split_df = pd.json_normalize(df.answers)\n","    df=df.drop('is_in', axis=1)\n","    df=df.drop('c_id', axis=1)\n","    df=df.drop('answers', axis=1)\n","    df['answer_text'] = split_df['text'].str.get(0)\n","    df['answer_start'] = split_df['answer_start'].str.get(0)\n","    df['language'] = ['hindi']*len(df) if hindi == True else ['tamil']*len(df)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:03.822892Z","iopub.status.busy":"2021-11-13T18:00:03.822205Z","iopub.status.idle":"2021-11-13T18:00:14.901191Z","shell.execute_reply":"2021-11-13T18:00:14.899684Z","shell.execute_reply.started":"2021-11-13T18:00:03.822832Z"},"trusted":true},"outputs":[],"source":["extdata_google_hi = pd.read_csv(hyperparams.external_google_hi,encoding = 'utf-8')\n","extdata_google_hi = process_google_translate(extdata_google_hi, hindi=True)\n","extdata_google_ta = pd.read_csv(hyperparams.external_google_ta,encoding = 'utf-8')\n","extdata_google_ta = process_google_translate(extdata_google_ta, hindi=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:14.903131Z","iopub.status.busy":"2021-11-13T18:00:14.902827Z","iopub.status.idle":"2021-11-13T18:00:14.929785Z","shell.execute_reply":"2021-11-13T18:00:14.928882Z","shell.execute_reply.started":"2021-11-13T18:00:14.903093Z"},"trusted":true},"outputs":[],"source":["# take few samples \n","extdata_1 = extdata_google_hi.sample(4500, random_state=42).reset_index(drop=True)\n","extdata_2 = extdata_google_ta.sample(4500, random_state=42).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:14.934026Z","iopub.status.busy":"2021-11-13T18:00:14.933751Z","iopub.status.idle":"2021-11-13T18:00:14.948842Z","shell.execute_reply":"2021-11-13T18:00:14.947812Z","shell.execute_reply.started":"2021-11-13T18:00:14.933998Z"},"trusted":true},"outputs":[],"source":["extdata = pd.concat([extdata, extdata_1, extdata_2])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:14.951096Z","iopub.status.busy":"2021-11-13T18:00:14.950459Z","iopub.status.idle":"2021-11-13T18:00:15.56096Z","shell.execute_reply":"2021-11-13T18:00:15.560005Z","shell.execute_reply.started":"2021-11-13T18:00:14.951047Z"},"trusted":true},"outputs":[],"source":["chaii_df = pd.read_csv(hyperparams.train_csv, encoding='utf-8')\n","# chaii_df = sklearn.utils.shuffle(chaii_df, random_state=4).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:15.562883Z","iopub.status.busy":"2021-11-13T18:00:15.562578Z","iopub.status.idle":"2021-11-13T18:00:15.568363Z","shell.execute_reply":"2021-11-13T18:00:15.567241Z","shell.execute_reply.started":"2021-11-13T18:00:15.562843Z"},"trusted":true},"outputs":[],"source":["# train_df, val_df = model_selection.train_test_split(chaii_df, test_size=0.1) #, random_state=hyperparams.seed) # hyperparams.seed\n","# train_df = train_df.reset_index(drop=True)\n","# val_df = val_df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:15.571114Z","iopub.status.busy":"2021-11-13T18:00:15.5702Z","iopub.status.idle":"2021-11-13T18:00:15.600808Z","shell.execute_reply":"2021-11-13T18:00:15.599705Z","shell.execute_reply.started":"2021-11-13T18:00:15.571067Z"},"trusted":true},"outputs":[],"source":["# train_df = pd.concat([train_df, extdata]).reset_index(drop=True)\n","train_df = pd.concat([chaii_df, extdata]).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:15.604607Z","iopub.status.busy":"2021-11-13T18:00:15.604347Z","iopub.status.idle":"2021-11-13T18:00:15.617094Z","shell.execute_reply":"2021-11-13T18:00:15.616125Z","shell.execute_reply.started":"2021-11-13T18:00:15.604575Z"},"trusted":true},"outputs":[],"source":["train_df = sklearn.utils.shuffle(train_df, random_state=42).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:18.819881Z","iopub.status.busy":"2021-11-13T18:00:18.819588Z","iopub.status.idle":"2021-11-13T18:00:18.824164Z","shell.execute_reply":"2021-11-13T18:00:18.823066Z","shell.execute_reply.started":"2021-11-13T18:00:18.819836Z"},"trusted":true},"outputs":[],"source":["# train_df = chaii_df.reset_index(drop=True) # complete data for training the model\n","# train_df = train_df.loc[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:19.595081Z","iopub.status.busy":"2021-11-13T18:00:19.594469Z","iopub.status.idle":"2021-11-13T18:00:19.601408Z","shell.execute_reply":"2021-11-13T18:00:19.600002Z","shell.execute_reply.started":"2021-11-13T18:00:19.595045Z"},"trusted":true},"outputs":[],"source":["print(len(train_df))#, len(val_df))"]},{"cell_type":"markdown","metadata":{},"source":["### Data pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:21.62807Z","iopub.status.busy":"2021-11-13T18:00:21.626776Z","iopub.status.idle":"2021-11-13T18:00:21.646913Z","shell.execute_reply":"2021-11-13T18:00:21.64563Z","shell.execute_reply.started":"2021-11-13T18:00:21.628021Z"},"trusted":true},"outputs":[],"source":["def prepare_chaii(data_df, tokenizer, test=False):\n","    # prepare_chaii takes in raw data and returns tokenized data \n","    # along with position of first token and last token in the answer_text\n","    \n","    # strip trailing and leading whitespaces in context, question, and (answer_text)?\n","    data_df.loc[:, 'context'] = data_df.loc[:, 'context'].apply(lambda sen : str(sen))\n","    data_df.loc[:, 'question'] = data_df.loc[:, 'question'].apply(lambda sen : str(sen).lstrip())\n","    if not test:\n","        data_df.loc[:, 'answer_text'] = data_df.loc[:, 'answer_text'].apply(lambda sen : str(sen))\n","    \n","    # question; context -- order is important, and is used in prediction stage to find whether predicted tokens seq_id is 0 (question) or 1 (context)\n","    data_tok = tokenizer(\n","        list(data_df['question']), list(data_df['context']),\n","        max_length=hyperparams.max_len, \n","        truncation='only_second',\n","        stride=hyperparams.overlap_len,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","    \n","    if test:\n","        return data_tok\n","    \n","    # data_df contains original raw data having question, context\n","    # data_tok contains tokenized data, where context might have split into multiple sentences \n","    # data_tok is a dict, containing keys : dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n","    # every value is a list, and no tensors here\n","    \n","    # adding two more keys that will contain the position of first token and last token in the answer_text\n","    data_tok['start_positions'], data_tok['end_positions'] = [], []\n","    \n","    n_sents = len(data_tok['input_ids'])\n","    map_id_sent2context = data_tok['overflow_to_sample_mapping'] # id means index! since input_ids means various inputs to the model\n","    map_offsets = data_tok['offset_mapping']\n","    assert len(map_offsets) == len(map_id_sent2context) == n_sents\n","    \n","    for input_id in range(n_sents):\n","        sent = data_tok['input_ids'][input_id]\n","        \n","        # get the answer_start and answer_text for this input_id using the id in data_df\n","        context_id = map_id_sent2context[input_id]\n","        answer_text = data_df.loc[context_id, 'answer_text']\n","        answer_start = data_df.loc[context_id, 'answer_start']\n","        answer_end = answer_start + len(answer_text) # will use this in next code block\n","        \n","        # check whether the answer is present in the current input_id or not using offsets\n","        qn_context_id = data_tok.sequence_ids(input_id)\n","        \n","            # first: get the start_idx_token and end_idx_token of context\n","        start_idx_token = qn_context_id.index(1)\n","        end_idx_token = len(qn_context_id) - qn_context_id[::-1].index(1) - 1\n","        \n","            # second: use the offsets for input_id to find if answer_start and answer_end are inside this chunk of context or not\n","        offset_map = map_offsets[input_id]\n","\n","        if answer_start >= offset_map[start_idx_token][0] and answer_end <= offset_map[end_idx_token][1]:\n","            # now finally get the idx_token for the first and last token in the answer_text\n","            while start_idx_token < len(sent) and answer_start >= offset_map[start_idx_token][0]:\n","                start_idx_token += 1\n","            while answer_end <= offset_map[end_idx_token][1]:\n","                end_idx_token -= 1\n","            \n","            data_tok['start_positions'].append(start_idx_token - 1)\n","            data_tok['end_positions'].append(end_idx_token + 1)\n","        \n","        else:\n","            cls_token_idx = sent.index(tokenizer.cls_token_id)\n","            assert cls_token_idx == 0\n","            data_tok['start_positions'].append(0) # cls token index\n","            data_tok['end_positions'].append(0) # cls token index\n","\n","    return data_tok     "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:22.682709Z","iopub.status.busy":"2021-11-13T18:00:22.682063Z","iopub.status.idle":"2021-11-13T18:00:22.692307Z","shell.execute_reply":"2021-11-13T18:00:22.690953Z","shell.execute_reply.started":"2021-11-13T18:00:22.682674Z"},"trusted":true},"outputs":[],"source":["class chaii_ka_data(Dataset):\n","    def __init__(self, data_df, tokenizer, test=False):\n","        super(chaii_ka_data, self).__init__()\n","        '''\n","            test = True means data_df without answer_text, answer_start\n","            data_df is the pandas dataframe containing context, question, ...        \n","        '''\n","        \n","        # tokenize data samples context;question, and create new samples if overflow\n","        # we need to do this apriori (and not in __getitem__ directly) because a datasample may create more samples upon tokenization\n","        self.reqd_keys = ['input_ids', 'attention_mask'] \n","        if not test:\n","            self.reqd_keys += ['start_positions', 'end_positions']\n","        self.data_tok = prepare_chaii(data_df, tokenizer, test=test)\n","    \n","    def __getitem__(self, input_id): # index is input_id as used in prepare_chaii()\n","        # sent = self.data_tok['input_ids'][input_id]\n","        # att_mask = self.data_tok['attention_mask'][input_id]\n","        # offset_map = self.data_tok['offset_mapping'][input_id]\n","        # start_idx_tok = self.data_tok['start_positions'][input_id]\n","        # end_idx_tok = self.data_tok['end_positions'][input_id]\n","        \n","        return {k: torch.tensor(v[input_id], dtype=torch.long) for k,v in self.data_tok.items() if k in self.reqd_keys}\n","    \n","    def __len__(self):\n","        return len(self.data_tok['input_ids'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:00:23.639871Z","iopub.status.busy":"2021-11-13T18:00:23.639597Z","iopub.status.idle":"2021-11-13T18:01:42.25652Z","shell.execute_reply":"2021-11-13T18:01:42.255349Z","shell.execute_reply.started":"2021-11-13T18:00:23.639842Z"},"trusted":true},"outputs":[],"source":["trainset = chaii_ka_data(train_df, tokenizer)\n","# valset = chaii_ka_data(val_df, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["#### model predictions (start and token index) to answer_text\n","this transformation requires original (raw) data_df, data_tok, and start, end logits (probabilities)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:01:42.259152Z","iopub.status.busy":"2021-11-13T18:01:42.25882Z","iopub.status.idle":"2021-11-13T18:01:42.289565Z","shell.execute_reply":"2021-11-13T18:01:42.287908Z","shell.execute_reply.started":"2021-11-13T18:01:42.259111Z"},"trusted":true},"outputs":[],"source":["def serve_chaii(test_df, testset, logits):\n","    assert len(logits[0]) == len(logits[1]) == len(testset)\n","    submission = {\n","        \"id\" : [],\n","        \"PredictionString\" : []\n","    }\n","    n_examples = len(test_df)\n","    # print(\"number of examples in test df\", n_examples)\n","    for example_idx in range(n_examples):\n","        # current example (or context) in the given test_df\n","        example_id = test_df.loc[example_idx, 'id']\n","        example_context = test_df.loc[example_idx, 'context']\n","\n","        # get all the features (or sents), start_logits, end_logits for the current example index\n","        data_tok = testset.data_tok\n","        map_id_sent2context = data_tok['overflow_to_sample_mapping']\n","        assert len(map_id_sent2context) == len(testset)\n","\n","        sents_first_idx = map_id_sent2context.index(example_idx)\n","        sents_last_idx = len(map_id_sent2context) - map_id_sent2context[::-1].index(example_idx) - 1\n","        assert (np.array(map_id_sent2context[sents_first_idx: sents_last_idx+1]) == example_idx).mean() == 1, set_trace()\n","\n","        sents = data_tok['input_ids'][sents_first_idx: sents_last_idx+1]\n","        start_logits = logits[0][sents_first_idx: sents_last_idx+1]\n","        end_logits = logits[1][sents_first_idx: sents_last_idx+1]\n","        sents_offset_mappings = data_tok['offset_mapping'][sents_first_idx: sents_last_idx+1]\n","        n_sents = len(sents)\n","        assert n_sents == len(start_logits) == len(end_logits)\n","        \n","        # get the answer_text from these sents using start_logits, end_logits\n","        # rank all possible answers for each sentence\n","        # then club all these answers from each sentence and take the best one as final predicted_answer\n","        # Also, consider the case when a sentence has no answer_text i.e. model predicts no answer\n","        pred_answers = []\n","        for local_idx in range(n_sents):\n","            sent = sents[local_idx]\n","            offset_mp = sents_offset_mappings[local_idx]\n","            start_lgts, end_lgts = start_logits[local_idx], end_logits[local_idx] # 384-dim list containing probabilities\n","\n","            # take the top 5 confident predictions of the model for start and end token indices\n","            top_x = hyperparams.top_x\n","            ranked_strt_tok_idxs = np.argsort(start_lgts)[::-1][:top_x].tolist()\n","            ranked_end_tok_idxs = np.argsort(end_lgts)[::-1][:top_x].tolist()\n","            \n","            seq_ids = data_tok.sequence_ids(sents_first_idx + local_idx)\n","\n","            # see which all are possible answers, and append\n","            for start_tok_idx in ranked_strt_tok_idxs:\n","                for end_tok_idx in ranked_end_tok_idxs:\n","                    # meaningless prediction\n","                    if (start_tok_idx > end_tok_idx) or (start_tok_idx>len(offset_mp)) or (end_tok_idx>len(offset_mp)):\n","                        continue\n","\n","                    # answer tokens NOT present in context, but in question\n","                    if seq_ids[start_tok_idx] == 0 or seq_ids[end_tok_idx] == 0 or seq_ids[start_tok_idx] == None or seq_ids[end_tok_idx] == None: # 0 denotes question since question; context, None means CLS, etc tokens\n","                        continue\n","                        \n","                    if end_tok_idx-start_tok_idx+1 > hyperparams.max_tok_in_ans:\n","                        continue\n","                    \n","                    if seq_ids[start_tok_idx] == 1 and seq_ids[end_tok_idx] == 1:\n","                        score = start_lgts[start_tok_idx] + end_lgts[end_tok_idx]\n","                        answer_text = example_context[offset_mp[start_tok_idx][0]: offset_mp[end_tok_idx][1]] # tokenizer.decode(sent[start_tok_idx: end_tok_idx+1])\n","                        pred_answers.append((score, answer_text))\n","\n","        if len(pred_answers) > 0:\n","            pred_answers = sorted(pred_answers, key=lambda element : element[0])[::-1]\n","            predicted_answer = pred_answers[0][1]\n","        else:\n","            print(\"empty answer predicted!!!\")\n","            predicted_answer = \"\"\n","\n","        submission['id'].append(example_id)\n","        submission['PredictionString'].append(predicted_answer)\n","\n","    assert len(submission['id']) == len(test_df)\n","    return submission"]},{"cell_type":"markdown","metadata":{},"source":["### model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:01:42.292231Z","iopub.status.busy":"2021-11-13T18:01:42.291672Z","iopub.status.idle":"2021-11-13T18:02:15.541429Z","shell.execute_reply":"2021-11-13T18:02:15.540443Z","shell.execute_reply.started":"2021-11-13T18:01:42.292171Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForQuestionAnswering.from_pretrained(hyperparams.model_name)"]},{"cell_type":"markdown","metadata":{},"source":["### training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T18:02:36.060873Z","iopub.status.busy":"2021-11-13T18:02:36.060546Z","iopub.status.idle":"2021-11-13T18:06:57.078771Z","shell.execute_reply":"2021-11-13T18:06:57.075098Z","shell.execute_reply.started":"2021-11-13T18:02:36.060833Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=hyperparams.output_dir, \n","    overwrite_output_dir=True, \n","    per_device_train_batch_size=hyperparams.batch_size,\n","#     per_device_eval_batch_size=hyperparams.batch_size,\n","#     evaluation_strategy=\"epoch\", \n","    learning_rate=3e-5,\n","    weight_decay=0.01,\n","    gradient_accumulation_steps=8,\n","    num_train_epochs=hyperparams.epochs,\n","    save_strategy=\"epoch\",\n","    warmup_ratio=0.1,\n",")\n","\n","data_collator = default_data_collator\n","\n","trainer = Trainer(\n","    model=model, args=training_args,\n","    train_dataset=trainset, \n","#     eval_dataset=valset,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation\n","Compute Jaccard's score for trainset, valset using saved model at each epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T17:07:49.373087Z","iopub.status.busy":"2021-11-13T17:07:49.372635Z","iopub.status.idle":"2021-11-13T17:07:49.38116Z","shell.execute_reply":"2021-11-13T17:07:49.379979Z","shell.execute_reply.started":"2021-11-13T17:07:49.373042Z"},"trusted":true},"outputs":[],"source":["# def jaccard(str1, str2): \n","#     a = set(str1.lower().split()) \n","#     b = set(str2.lower().split())\n","#     c = a.intersection(b)\n","#     return float(len(c)) / (len(a) + len(b) - len(c))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T17:07:49.630068Z","iopub.status.busy":"2021-11-13T17:07:49.628395Z","iopub.status.idle":"2021-11-13T17:07:49.638697Z","shell.execute_reply":"2021-11-13T17:07:49.637975Z","shell.execute_reply.started":"2021-11-13T17:07:49.630018Z"},"trusted":true},"outputs":[],"source":["# def compute_jaccard(pred_df, gt_df):\n","#     num_examples = 0\n","#     score = 0\n","#     for idx, example_id in enumerate(gt_df.loc[:,'id']):\n","#         gt_answer = gt_df.loc[idx, 'answer_text']\n","#         pred_answer = pred_df.loc[pred_df.loc[:, 'id'] == example_id].reset_index(drop=True).loc[0, 'PredictionString']\n","#         # print(gt_answer, pred_answer)\n","#         score += jaccard(gt_answer, pred_answer)\n","#         num_examples += 1\n","        \n","#     score /= num_examples\n","#     return score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2021-11-13T18:02:15.995072Z","iopub.status.idle":"2021-11-13T18:02:15.995889Z","shell.execute_reply":"2021-11-13T18:02:15.99561Z","shell.execute_reply.started":"2021-11-13T18:02:15.995575Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# model_checkpoints = os.listdir(hyperparams.output_dir)\n","# model_checkpoints1 = sorted(model_checkpoints, key=lambda element : int(element[11:]))[::-1]\n","# model_checkpoint = model_checkpoints1[0]\n","# train_scores, val_scores = [], []\n","# for cp_id, model_checkpoint in enumerate(model_checkpoints):\n","#     if model_checkpoint[:5] != \"check\":\n","#         continue\n","#     # load the model\n","#     model = AutoModelForQuestionAnswering.from_pretrained(os.path.join(hyperparams.output_dir, model_checkpoint))\n","#     trainer = Trainer(\n","#         model=model\n","#     )\n","    \n","# #     logits = trainer.predict(trainset).predictions\n","# #     submission = serve_chaii(train_df, trainset, logits)\n","# #     pred_df = pd.DataFrame(submission)\n","# #     train_score = compute_jaccard(pred_df, train_df)\n","# #     train_scores.append(train_score)\n","\n","#     logits = trainer.predict(valset).predictions\n","#     submission = serve_chaii(val_df, valset, logits)\n","#     pred_df = pd.DataFrame(submission)\n","#     val_score = compute_jaccard(pred_df, val_df)\n","#     val_scores.append(val_score)\n","    \n","    \n","#     # print(model_checkpoint, \"train:\", train_score, \"val:\", val_score)\n","# print(model_checkpoints)\n","# # print(train_scores)\n","# print(val_scores)"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction of textual answers\n","1. generate submission.csv containing \"id\", \"PredictionString\" columns\n","2. Use Trainer API for predict instead of trainer.model(\\*\\*batch) as it handles batching, and CPU-GPU on its own\n","3. trainer.predict(testset) gives the start and end logits for all the input features in the test set\n","4. for each example in the test_df, select the best answer from its features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T14:08:36.767426Z","iopub.status.busy":"2021-11-13T14:08:36.766992Z","iopub.status.idle":"2021-11-13T14:08:36.773697Z","shell.execute_reply":"2021-11-13T14:08:36.772975Z","shell.execute_reply.started":"2021-11-13T14:08:36.767389Z"},"trusted":true},"outputs":[],"source":["model_checkpoints = os.listdir(hyperparams.output_dir)\n","model_checkpoints1 = sorted(model_checkpoints, key=lambda element : int(element[11:]))[::-1]\n","model_checkpoint = model_checkpoints1[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T11:20:26.925177Z","iopub.status.busy":"2021-11-13T11:20:26.924788Z","iopub.status.idle":"2021-11-13T11:21:04.239519Z","shell.execute_reply":"2021-11-13T11:21:04.238791Z","shell.execute_reply.started":"2021-11-13T11:20:26.925141Z"},"trusted":true},"outputs":[],"source":["# load the model\n","model = AutoModelForQuestionAnswering.from_pretrained(os.path.join(hyperparams.output_dir, model_checkpoint))\n","trainer = Trainer(\n","    model=model\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T11:21:04.241546Z","iopub.status.busy":"2021-11-13T11:21:04.241298Z","iopub.status.idle":"2021-11-13T11:21:04.535425Z","shell.execute_reply":"2021-11-13T11:21:04.534Z","shell.execute_reply.started":"2021-11-13T11:21:04.241513Z"},"trusted":true},"outputs":[],"source":["# load the dataset\n","test_df = pd.read_csv(hyperparams.test_csv, encoding='utf-8') # uncomment this for submission\n","# test_df = pd.read_csv(hyperparams.train_csv, encoding='utf-8').loc[:10] # comment this for submission\n","testset = chaii_ka_data(test_df, tokenizer, test=True)\n","# testloader = DataLoader(testset, batch_size=16)\n","# next(iter(testloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T11:21:04.537541Z","iopub.status.busy":"2021-11-13T11:21:04.537011Z","iopub.status.idle":"2021-11-13T11:21:08.200347Z","shell.execute_reply":"2021-11-13T11:21:08.199648Z","shell.execute_reply.started":"2021-11-13T11:21:04.537501Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# pass the complete testset in trainer API\n","# the API will automatically do batch-wise prediction\n","# start, end logits are accessible using model_output.predictions[0],[1]\n","# if the testset has labels, then model_output.label_ids contains them\n","# model_output.metrics = {'test_runtime': 10.6385, 'test_samples_per_second': 128.684, 'test_steps_per_second': 16.168}\n","model_output = trainer.predict(testset)\n","logits = model_output.predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T11:21:08.204159Z","iopub.status.busy":"2021-11-13T11:21:08.203891Z","iopub.status.idle":"2021-11-13T11:21:08.233974Z","shell.execute_reply":"2021-11-13T11:21:08.233332Z","shell.execute_reply.started":"2021-11-13T11:21:08.204126Z"},"trusted":true},"outputs":[],"source":["submission = serve_chaii(test_df, testset, logits)\n","submission_df = pd.DataFrame(submission)\n","# submission_df.loc[:, 'PredictionString'] = submission_df.loc[:, 'PredictionString'].apply(lambda ans: \"\\\"\" + str(ans) + \"\\\"\")\n","submission_df.to_csv('submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### references\n","1. https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n","2. https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
